{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab5a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\coolc\\anaconda3\\envs\\RAG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llmsherpa.readers import LayoutPDFReader\n",
    "import openai\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a9901",
   "metadata": {},
   "source": [
    "## RAG & parsing pdfs\n",
    "\n",
    "### What is RAG?\n",
    "- RAG stands for Retrieval-Augmented Generation.\n",
    "- It is a technique that combines retrieval of relevant document information with generative models to enhance the quality and relevance of generated responses.\n",
    "- RAG systems typically retrieve relevant documents from a knowledge base or corpus and use them to inform the generation of responses.\n",
    "\n",
    "### Parsing PDFs\n",
    "- Parsing PDFs is more challenging than parsing text like docx or txt files due to the complex structure of PDF documents.\n",
    "    - What is complex about pdfs?\n",
    "        - Pds are meant to be hard to edit, and highly formatted.\n",
    "        - They can contain images, tables, and various layouts that make text extraction difficult.\n",
    "        - pdfs are do not contain accessible text, but rather a visual representation of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e3549",
   "metadata": {},
   "source": [
    "### The `nlm-ingestor` parser:\n",
    "- We are going to use a parser specifically designed for RAG and PDF documents called `nlm-ingestor` from the `llmsherpa`module. Why?\n",
    "    - It is designed to handle the complexities of PDF documents and data structures useful for the Retrieval portion of RAG.\n",
    "    - It combines OCR (Optical Character Recognition), the text layer of PDFs, and the font objects (text co-ordinates/boundary box, graphics and font data) to parse the structure and content of PDF documents.\n",
    "    - It can also handle other document formats like docx, txt, and images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70485a06",
   "metadata": {},
   "source": [
    "### Install requirements:\n",
    "- The easiest way to install and run the parser server is with Docker.\n",
    "    - Why Docker?\n",
    "        - Problem: I (Cameron) write python on Windows, and the parser uses `libxml2` and `libxslt` as dependencies which require building from source with tools not normally included in Windows by default. More information is found [here](https://lxml.de/installation.html#source-builds-on-ms-windows).\n",
    "\n",
    "        - Solution: The Windows Subsystem for Linux (WSL2) + Docker. Docker is a container manager that can pull and build github projects and run them in a sort of virtual machine. With WSL2 we install a Linux distribution (default is Ubuntu) that Docker can use as its backend instead of Windows.\n",
    "\n",
    "### Install process:\n",
    "1. setup WSL2 if you are on Windows, instructions are found [here](https://learn.microsoft.com/en-us/windows/wsl/install). \n",
    "2. Install Docker using the setup guide, instructions are found [text](https://docs.docker.com/desktop/features/wsl/#turn-on-docker-desktop-wsl-2).\n",
    "3. Install the the `nlm-ingestor`in the Docker UI's terminal Window with these commands:\n",
    "    - get the current parser build: `docker pull ghcr.io/nlmatics/nlm-ingestor:latest`\n",
    "    - Run the server: `docker run -p 5010:5001 ghcr.io/nlmatics/nlm-ingestor:latest`\n",
    "    - nlm-ingestor readme [link](https://github.com/nlmatics/nlm-ingestor?tab=readme-ov-file#about)\n",
    "4. now install `pip install llmsherpa`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99200e99",
   "metadata": {},
   "source": [
    "### Step 1: parser through the server\n",
    "The code below will parse a PDF file through the server and return the parsed data as a JSON object. THE SERVER MUST BE RUNNING FOR THIS TO WORK!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "496d1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmsherpa_api_url = \"http://localhost:5010/api/parseDocument?renderFormat=all\" # local API endpoint\n",
    "pdf_url = \"ExampleRFPs/GoodFit/IETSS DRAFT PWS v2 for RFI FINAL to POST_no_contents.pdf\"\n",
    "# \"ExampleRFPs/GoodFit/IETSS DRAFT PWS v2 for RFI FINAL to POST.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdf\n",
    "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "doc = pdf_reader.read_pdf(pdf_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b2a76",
   "metadata": {},
   "source": [
    "The basic idea of the Retrieval step is to ask questions of the document. To do this we will use the `sentence_transformers` library to embed the questions and the document text, and then use cosine similarity to find the most relevant parts of the document that match the question. This is called \"semantic search\". As the name implies, it is a search for the meaning of the text rather than exact matches of words. The parser will return a `Document` object that contains the document and the parse data as a tree node structure. We will use the `Document` object to extract the text and embed it for semantic search. We need to:\n",
    "\n",
    "### RAG step 2: Extracting text with context\n",
    "- extract text and provide context about the document sections and structure that will be used for the semantic search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in doc.tables():\n",
    "    print(\"Table:\")\n",
    "    print(table.to_context_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b678ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in doc.sections():\n",
    "    print(f\"Section: {section.title}\")\n",
    "    print(f\"Children: {[child.to_text() for child in section.children]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3781a",
   "metadata": {},
   "source": [
    "The code cell below shows how the nlm server chunks the pdf with smart chunking. 'include_section_info' is set to true, this lets us see the section information for the context of each chunk. Each chunk is a logical unit of text that is semantically meaningful like paragraphs, tables, or list items. \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,chunk in enumerate(doc.chunks()):\n",
    "    print(f\"-----Chunk {i}-----\")\n",
    "    # print(chunk.to_text(include_children=True, recurse=True))\n",
    "    print(chunk.to_context_text(include_section_info=True))\n",
    "    print(\"----------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf459c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,chunk in enumerate(doc.sections()):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk.to_context_text(include_section_info=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f2bc9",
   "metadata": {},
   "source": [
    "As you can see, there are many options for the parser to control how to get structured data from the PDF including sectioning context in smart chunking, and the ability to include or exclude images, tables, and other elements. \n",
    "\n",
    "The next step requires us to choose a method best suited for the type of query we want to perform. For example, if we want to ask general questions about the document content, we can provide section titles and possibly child nodes to help guide the semantic search. If we want to ask specific questions about a sections paragraphs, we probably want to include the child nodes to get the most relevant text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55fdfc",
   "metadata": {},
   "source": [
    "### RAG step 3: Semantic search\n",
    "- embed the text and the questions using the `sentence_transformers` library.\n",
    "- calculate the cosine similarity between the embedded questions and the embedded text.\n",
    "- return the most relevant sections of the document that match the questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b086b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e80cc0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the paragraphs into embeddings.\n",
    "sections = []\n",
    "for section in doc.sections():\n",
    "    sections.append(section.to_context_text(include_section_info=True))\n",
    "    # sections.append(section.title)\n",
    "doc_embeddings = model.encode(sections, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3065fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: which section is about the objective?\n",
      "Result 0:\n",
      "1.2 OIT SERVICES AND METHODOLOGY > Figure 1.2-3: Service Offerings and the SDLC and SACI phases\n",
      "1.2.1 Services Executed During SDLC and SACI Planning Phase\n",
      "Score: 0.3704\n",
      "\n",
      "Query: which section is about the objective?\n",
      "Result 1:\n",
      "1.2 OIT SERVICES AND METHODOLOGY\n",
      "Figure 1.2-3: Service Offerings and the SDLC and SACI phases\n",
      "Score: 0.3652\n",
      "\n",
      "Query: which section is about the objective?\n",
      "Result 2:\n",
      "\n",
      "1.2 OIT SERVICES AND METHODOLOGY\n",
      "Score: 0.3279\n",
      "\n",
      "Query: which section is about the objective?\n",
      "Result 3:\n",
      "\n",
      "3.0 SCOPE OF WORK\n",
      "Score: 0.3192\n",
      "\n",
      "Query: which section is about the objective?\n",
      "Result 4:\n",
      "1.2 OIT SERVICES AND METHODOLOGY > Figure 1.2-3: Service Offerings and the SDLC and SACI phases\n",
      "1.2.2 Services Executed During SDLC and SACI Test Services Execution Phase\n",
      "Score: 0.3170\n",
      "\n",
      "Query: which section is about the background?\n",
      "Result 0:\n",
      "\n",
      "1.0 BACKGROUND\n",
      "Score: 0.5665\n",
      "\n",
      "Query: which section is about the background?\n",
      "Result 1:\n",
      "1.2 OIT SERVICES AND METHODOLOGY > Figure 1.2-3: Service Offerings and the SDLC and SACI phases\n",
      "1.2.1 Services Executed During SDLC and SACI Planning Phase\n",
      "Score: 0.2110\n",
      "\n",
      "Query: which section is about the background?\n",
      "Result 2:\n",
      "\n",
      "4.2 PLACE OF PERFORMANCE\n",
      "Score: 0.2093\n",
      "\n",
      "Query: which section is about the background?\n",
      "Result 3:\n",
      "\n",
      "POINTS OF CONTACT\n",
      "Score: 0.2070\n",
      "\n",
      "Query: which section is about the background?\n",
      "Result 4:\n",
      "1.2 OIT SERVICES AND METHODOLOGY > Figure 1.2-3: Service Offerings and the SDLC and SACI phases > 1.2.3 Processes Executed in Support of Monitoring and Control\n",
      "1.2.3.1 Workload Forecasting\n",
      "Score: 0.1931\n",
      "\n",
      "Query: which section is about the work scope?\n",
      "Result 0:\n",
      "\n",
      "3.0 SCOPE OF WORK\n",
      "Score: 0.7132\n",
      "\n",
      "Query: which section is about the work scope?\n",
      "Result 1:\n",
      "5.1 PROJECT MANAGEMENT (FFP) > 5.1.3 Reporting Requirements & Management Meetings\n",
      "A. Monthly Progress Report\n",
      "Score: 0.4320\n",
      "\n",
      "Query: which section is about the work scope?\n",
      "Result 2:\n",
      "5.1 PROJECT MANAGEMENT (FFP)\n",
      "5.1.3 Reporting Requirements & Management Meetings\n",
      "Score: 0.4212\n",
      "\n",
      "Query: which section is about the work scope?\n",
      "Result 3:\n",
      "5.1 PROJECT MANAGEMENT (FFP) > 5.1.3 Reporting Requirements & Management Meetings\n",
      "Deliverable:\n",
      "Score: 0.4105\n",
      "\n",
      "Query: which section is about the work scope?\n",
      "Result 4:\n",
      "5.1 PROJECT MANAGEMENT (FFP) > 5.1.1 Contractor Project Management Plan\n",
      "A. Contractor Project Management Plan\n",
      "Score: 0.3742\n",
      "\n",
      "Query: which section is about the deliverables?\n",
      "Result 0:\n",
      "\n",
      "SCHEDULE FOR DELIVERABLES\n",
      "Score: 0.6379\n",
      "\n",
      "Query: which section is about the deliverables?\n",
      "Result 1:\n",
      "\n",
      "5.0 SPECIFIC TASKS AND DELIVERABLES\n",
      "Score: 0.5657\n",
      "\n",
      "Query: which section is about the deliverables?\n",
      "Result 2:\n",
      "5.3 TESTING AND TECHNOLOGY SUPPORT > Deliverables:\n",
      "B. Test Environment Logical Requirements Document\n",
      "Score: 0.4907\n",
      "\n",
      "Query: which section is about the deliverables?\n",
      "Result 3:\n",
      "5.3 TESTING AND TECHNOLOGY SUPPORT\n",
      "Deliverables:\n",
      "Score: 0.4898\n",
      "\n",
      "Query: which section is about the deliverables?\n",
      "Result 4:\n",
      "SCHEDULE FOR DELIVERABLES\n",
      "TSEI – Revised Patch Management Plan\n",
      "Score: 0.4823\n",
      "\n",
      "Query: which section is about the performance metrics?\n",
      "Result 0:\n",
      "\n",
      "4.2 PLACE OF PERFORMANCE\n",
      "Score: 0.6076\n",
      "\n",
      "Query: which section is about the performance metrics?\n",
      "Result 1:\n",
      "\n",
      "4.1 PERFORMANCE PERIOD\n",
      "Score: 0.5562\n",
      "\n",
      "Query: which section is about the performance metrics?\n",
      "Result 2:\n",
      "\n",
      "4.0 PERFORMANCE DETAILS\n",
      "Score: 0.5399\n",
      "\n",
      "Query: which section is about the performance metrics?\n",
      "Result 3:\n",
      "5.2 TEST AND EVALUATION SUPPORT (T&M) > 5.2.6 Execute Test Plan, Analyze Results, & Report Findings > Deliverable: > A. Test Analysis Summary\n",
      "5.2.6.2 Performance Testing\n",
      "Score: 0.4480\n",
      "\n",
      "Query: which section is about the performance metrics?\n",
      "Result 4:\n",
      "1.2 OIT SERVICES AND METHODOLOGY > Figure 1.2-3: Service Offerings and the SDLC and SACI phases > 1.2.3 Processes Executed in Support of Monitoring and Control\n",
      "1.2.3.1 Workload Forecasting\n",
      "Score: 0.4396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"which section is about the objective?\",\n",
    "    \"which section is about the background?\",\n",
    "    \"which section is about the work scope?\",\n",
    "    \"which section is about the deliverables?\",\n",
    "    \"which section is about the performance metrics?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    similarities = model.similarity(query_embedding, doc_embeddings)[0]\n",
    "    # top 5 results\n",
    "    top_indices = torch.topk(similarities, 5)\n",
    "    for i in range(len(top_indices[0])):\n",
    "        print(f\"Query: {query}\\nResult {i}:\\n{sections[top_indices[1][i]]}\\nScore: {top_indices[0][i].item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333284cb",
   "metadata": {},
   "source": [
    "### RAG step 4: Summarization\n",
    "- summarize the relevant sections using a generative model.\n",
    "\n",
    "Example code using the `transformers` huggingface library to summarize the text is shown below. This will use a pre-trained model to generate a summary of the relevant sections of the document.\n",
    "```python\n",
    "from transformers import pipeline\n",
    "# Load a summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", \n",
    "                      model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def summarize_text(text: str) -> str:\n",
    "    \"\"\"Summarize the input text using the summarization pipeline.\"\"\"\n",
    "    # Adjust max_length and min_length as needed but dont exceed the attention limit of the model\n",
    "    summary = summarizer(text, max_length=len(text)/2, min_length=30, do_sample=False) \n",
    "    return summary[0]['summary_text'] if summary else \"No summary available.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        # input_text will be the chunk of text to summarize in the actual application\n",
    "        input_text = '''The Contractor will be required to design, develop, or operate a system of records on individuals, to accomplish \n",
    "                        an agency function subject to the Privacy Act of 1974, Public Law 93-579, December 31, 1974 (5 U.S.C. 552a) and \n",
    "                        applicable agency regulations. Violation of the Act may involve the imposition of criminal penalties.'''\n",
    "        summary = summarize_text(input_text)\n",
    "        print(\"Summary:\")\n",
    "        print(summary)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7295e5d2",
   "metadata": {},
   "source": [
    "Example for how we can target an online api like the openai api to summarize the text is shown below. This will use the OpenAI API to generate a summary of the relevant sections of the document.\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b027a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"your-openai-api-key\"\n",
    "index = VectorStoreIndex([])\n",
    "for chunk in doc.chunks():\n",
    "    index.insert(Document(text=chunk.to_context_text()), extra_info={})\n",
    "retriever = index.as_retriever()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
